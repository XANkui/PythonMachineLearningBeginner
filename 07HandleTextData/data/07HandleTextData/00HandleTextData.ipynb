{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b09349d-e757-4a5c-abd1-324f4395f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 PATH 列表\n",
      "卷序列号为 2109-26B9\n",
      "E:\\PROJECTS\\PYCHARMPROJECTS\\MACHINELEARNINGPROJECT\\07HANDLETEXTDATA\\DATA\\07HANDLETEXTDATA\\DATA\\ACLIMDB\n",
      "无效的路径 - \\PROJECTS\\PYCHARMPROJECTS\\MACHINELEARNINGPROJECT\\07HANDLETEXTDATA\\DATA\\07HANDLETEXTDATA\\DATA\\ACLIMDB\n",
      "没有子文件夹 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tree data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58ad640-ecb6-436f-980d-84316eb045b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'data/aclImdb/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_files\n\u001b[1;32m----> 3\u001b[0m reviews_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/aclImdb/train/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# load_files返回一个Bunch对象，其中包含训练文本和训练标签\u001b[39;00m\n\u001b[0;32m      5\u001b[0m text_train, y_train \u001b[38;5;241m=\u001b[39m reviews_train\u001b[38;5;241m.\u001b[39mdata, reviews_train\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\MachineLearningProject\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Program Files\\Anaconda3\\envs\\MachineLearningProject\\Lib\\site-packages\\sklearn\\datasets\\_base.py:266\u001b[0m, in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state, allowed_extensions)\u001b[0m\n\u001b[0;32m    262\u001b[0m target_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    263\u001b[0m filenames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    265\u001b[0m folders \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 266\u001b[0m     f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontainer_path\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m isdir(join(container_path, f))\n\u001b[0;32m    267\u001b[0m ]\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m categories \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     folders \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folders \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m categories]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'data/aclImdb/train/'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "# load_files返回一个Bunch对象，其中包含训练文本和训练标签\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de43448-b423-4011-a898-1d6347e25413",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c970d9-19fd-4fad-a828-26d6db1ffa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b842660-19df-49a1-a78a-110cf2ec470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d94fda-2919-49f9-a5a0-d7d862ee7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2a9f7-d8e6-48c1-a63c-431359db60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467ebf3-0fc2-411c-852e-57843d8cc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b235426-c2d4-4115-942a-ca0c24e3f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda6b08-36a0-45d1-a7f3-25929d73da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
    "    bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cec45f-f0e7-4f48-ac8b-7714ab59135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b57601-caaa-4eeb-8c7d-ab8bd0ab22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b819fa2-8eb3-4057-a692-5e9b6d4e0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ec4e6-247e-4939-8527-032c1b9d9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add85dbe-5c29-440a-a9ed-125bda36611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = vect.transform(text_test)\n",
    "print(\"{:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538de7a-0841-4461-ad37-bde67be0500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ea2c5-d0f7-435c-9005-e180fce908e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61ea7a-0bf7-429c-987c-7289299bface",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7e673-5c82-4d9d-8c66-a94b16cd6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119b0ab-340b-4568-859b-4979b199fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定stop_words=\"english\"将使用内置列表。\n",
    "# 我们也可以扩展这个列表并传入我们自己的列表。\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43795b3e-7cfe-4935-a909-4ffc184c3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781aeb3-802b-4cf1-8e22-2d2cb1d1f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5),\n",
    "                     LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21a290-8af2-42c5-b496-c627ca86153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# 变换训练数据集\n",
    "X_train = vectorizer.transform(text_train)\n",
    "# 找到数据集中每个特征的最大值\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "# 获取特征名称\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "    feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"Features with highest tfidf: \\n{}\".format(\n",
    "    feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe693fc6-fdfb-4d76-8cef-c3bdcd240beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "    feature_names[sorted_by_idf[:100]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525ab97-800f-454a-8adc-e6cd2024c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mglearn.tools.visualize_coefficients(\n",
    "    grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\n",
    "    feature_names, n_top_features=40)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Images/00HandleTextData-01.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143e0a0-7cd8-4dbf-b611-5a25820eadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bards_words:\\n{}\".format(bards_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662bf41-1735-46a1-8e1c-b1f336cf230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b353d-4686-4fcb-96a6-797541ee5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29bdfd-4e9a-4440-bbfc-0a9c78277c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed data (dense):\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b3606-759e-4f49-8729-555015deaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafe5f2-f470-4938-9898-2467d10e2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "# 运行网格搜索需要很长时间，因为网格相对较大，且包含三元分词\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29243bf9-5001-4521-a986-7516086ec511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawHeatmap(scores, xlabel,\n",
    "    ylabel='True label', fmt='{:0.2f}', colorbar=None):\n",
    "    # 反转scores数组的行顺序，以便(0, 0)在左下角  \n",
    "    scores = scores[::-1, :]  \n",
    "      \n",
    "    # 绘制热图  \n",
    "    fig, ax = plt.subplots()  \n",
    "    if(colorbar is None):\n",
    "        colorbar = scores\n",
    "    cax = ax.matshow(colorbar, cmap='viridis')  \n",
    "    fig.colorbar(cax)  \n",
    "      \n",
    "    # 假设param_grid['gamma']和param_grid['C']已经定义  \n",
    "    # 这里我们使用字符串列表作为示例  \n",
    "    param_grid = {  \n",
    "        'gamma': ['0.001','0.01', '0.1', '1', '10', '100'],  \n",
    "        'C':   ['(1,1)','(1,2)','(1,3)']\n",
    "    }  \n",
    "      \n",
    "    # 设置xticks和yticks的位置  \n",
    "    ticks_x = np.arange(len(param_grid['gamma']))  \n",
    "    ticks_y = np.arange(len(param_grid['C']))[::-1]  # 反转yticks的顺序以匹配反转的scores  \n",
    "      \n",
    "    # 设置xticklabels和yticklabels  \n",
    "    ax.set_xticks(ticks_x)  \n",
    "    ax.set_xticklabels(param_grid['gamma'])  \n",
    "    ax.xaxis.set_tick_params(rotation=45)  \n",
    "      \n",
    "    ax.set_yticks(ticks_y)  \n",
    "    ax.set_yticklabels(param_grid['C'])  \n",
    "      \n",
    "    # 设置x轴和y轴的标签  \n",
    "    ax.set_xlabel(xlabel)  \n",
    "    ax.set_ylabel(ylabel)  \n",
    "      \n",
    "    # 添加数值到每个单元格  \n",
    "    for (i, j), z in np.ndenumerate(scores):  \n",
    "        ax.text(j, i, fmt.format(z), ha='center', va='center')  \n",
    "\n",
    "\n",
    "# 从网格搜索中提取分数\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n",
    "# 热图可视化\n",
    "# heatmap = mglearn.tools.heatmap(\n",
    "#    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n",
    "#    xticklabels=param_grid['logisticregression__C'],\n",
    "#    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\n",
    "\n",
    "drawHeatmap(scores,xlabel=\"C\", ylabel=\"ngram_range\",fmt='{:0.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Images/00HandleTextData-02.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdda427-529d-4b44-861d-1c6548500a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征名称与系数\n",
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "coef = grid.best_estimator_.named_steps['logisticregression'].coef_\n",
    "mglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Images/00HandleTextData-03.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba32da-9264-4b8d-ac13-47c4da84f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到三元分词特征\n",
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
    "# 仅将三元分词特征可视化\n",
    "mglearn.tools.visualize_coefficients(coef.ravel()[mask],\n",
    "                                     feature_names[mask], n_top_features=40)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Images/00HandleTextData-04.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8bd79-cbc5-4abe-ac3b-671c9918a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# 加载spacy的英语模型\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "# 将nltk的Porter词干提取器实例化\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# 定义一个函数来对比spacy中的词形还原与nltk中的词干提取\n",
    "def compare_normalization(doc):\n",
    "    # 在spacy中对文档进行分词\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    # 打印出spacy找到的词元\n",
    "    print(\"Lemmatization:\")\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    # 打印出Porter词干提取器找到的词例\n",
    "    print(\"Stemming:\")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ebe7c-5a79-4586-9add-47c3c8003705",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday, \"\n",
    "                       \"I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759e2c6-168e-4fa4-82cc-ed058287a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 技术细节：我们希望使用由CountVectorizer所使用的基于正则表达式的分词器，\n",
    "# 并仅使用spacy的词形还原。\n",
    "# 为此，我们将en_nlp.tokenizer（spacy分词器）替换为基于正则表达式的分词。\n",
    "# 引入 CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "# 在CountVectorizer中使用的正则表达式\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "\n",
    "# 加载spacy语言模型，并保存旧的分词器\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "# 将分词器替换为前面的正则表达式\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "    regexp.findall(string))\n",
    "\n",
    "# 用spacy文档处理管道创建一个自定义分词器\n",
    "# （现在使用我们自己的分词器）\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document, entity=False, parse=False)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# 利用自定义分词器来定义一个计数向量器\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74458bbc-90e9-4a4a-95b3-316f959f139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用带词形还原的CountVectorizer对text_train进行变换\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
    "\n",
    "# 标准的CountVectorizer，以供参考\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42215d-9806-4cb7-bb14-78ae9f32d58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
